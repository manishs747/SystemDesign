 Leadership Craft 

1. Can you walk me through a time you had to plan and execute an engineering project end to end, from scoping & estimation to delivery?
    * What problem did this project solve and how was it identified?
    * Did this project require you to work across role/team/org boundaries? How did this impact your approach to planning?
    * What were the risks? How did you plan to mitigate those risks?
    * How did you “prove” the design is solving the right problem? Early integration, incremental delivery and customer feedback.
        * How did you measure and communicate success?

ANS: 

Absolutely — that’s a **fantastic Staff Engineer–level story** and fits the Leadership Craft question perfectly.
Here’s a polished **STAR-format answer** based on your Swiggy payment service migration, incorporating the gradual rollout, load testing, and infra setup you mentioned.

---

## **Leadership Craft Example – Swiggy Payment Service Migration**

### **Situation & Problem**

At Swiggy, the **payment module was tightly coupled with the checkout service**, making it hard to scale independently.
During peak events (festivals, cricket matches), spikes in payment traffic would slow down or even bring down checkout — causing **cart abandonment, payment failures, and lost revenue**.

Product & SRE teams identified this as a **business-critical reliability risk**, as any downtime in checkout meant direct impact to order volume and revenue.

---

### **Task**

I was responsible for **decoupling the payment module** into a standalone **Payment Service**, building a robust infrastructure around it, and rolling it out with minimal customer impact.

My goals:

* Design the new service architecture and database schema.
* Ensure backward compatibility with checkout while migrating traffic gradually.
* Test for scale (3× peak traffic) and avoid regressions in payment success rate.

---

### **Actions**

#### **1. Scoping & Estimation**

* **Analyzed Dependencies:** Mapped out tightly coupled modules inside checkout (cart state validation, discount engine, order placement triggers).
* Broke the migration into **phases:**

  1. Service skeleton & database setup
  2. API extraction (one payment method at a time)
  3. Integration with checkout behind feature flags
  4. Gradual traffic ramp-up
* Estimated engineering effort and aligned with Product & SRE on timelines and rollback plans.

---

#### **2. Cross-Team Collaboration**

* Worked with **Product & Payments Ops** to prioritize which payment methods to migrate first.
* Coordinated with **SRE & DevOps** to provision infra (autoscaling groups, monitoring, alerting).
* Partnered with **QA** to define regression test suites for each API endpoint.

This required **detailed RFCs** and design docs, which I presented in cross-team design reviews to ensure alignment.

---

#### **3. Risk Identification & Mitigation**

| **Risk**                             | **Mitigation**                                                                            |
| ------------------------------------ | ----------------------------------------------------------------------------------------- |
| Payment failures during rollout      | Feature flagging & gradual rollout: 1% → 10% → 50% → 100%                                 |
| Latency increase (extra network hop) | Benchmarked APIs, added connection pooling & async callbacks                              |
| Infra under-provisioned              | Load tested with **JMeter** at 3× peak traffic before ramp-up                             |
| Rollback complexity                  | Built kill-switch to route traffic back to monolith immediately if error rate > threshold |

---

#### **4. Early Feedback & Incremental Delivery**

* Migrated **one payment method at a time** (e.g., UPI first, then cards, wallets, etc.) to limit blast radius.
* Used **A/B testing** and gradually ramped traffic (1%, 10%, 50%, 100%) while monitoring latency and success rate.
* Conducted **synthetic monitoring tests** on each API before enabling it for real users.

---

#### **5. Proving Success**

* **Metrics Tracked:**

  * Payment success rate (must stay at or above baseline)
  * P95 payment latency
  * Error rate (alert if >2%)
  * Infra utilization (CPU, memory, DB connections)
* **Communication:**

  * Shared daily dashboards with Product & SRE teams during rollout.
  * Published weekly updates to leadership showing % traffic migrated and stability metrics.

---

### **Result**

* **100% traffic migrated** to the new Payment Service within 6 weeks with **zero critical incidents**.
* **Payment success rate improved by \~25%** due to retries, circuit breakers, and improved isolation.
* Checkout service latency dropped since payments no longer blocked order placement.
* Infra now scales independently for payments, avoiding cascading failures during high-traffic events.
* Knowledge from this migration was documented and later used as a blueprint for decoupling other modules.

---

### **Reflection**

This project reinforced that **gradual rollout + observability** is key to de-risking large-scale migrations.
By designing feature flags, A/B testing, and rollback paths upfront, we avoided customer impact while executing a complex cutover. It also improved trust with product and SRE teams, as they had clear visibility into metrics at every stage.

---

### Why This Works for Leadership Craft

✅ **End-to-end ownership:** Scoping, infra setup, design, delivery, validation
✅ **Cross-functional alignment:** Product, SRE, DevOps, QA all involved
✅ **Risk-driven planning:** Kill-switch, staged rollout, load tests
✅ **Metrics-first approach:** Latency, success rate, error budget tracked & communicated
✅ **Scalable impact:** Architecture now supports future growth and feature velocity


WALMART EXAMPLE:
Absolutely — that Walmart Labs search migration is another **excellent Staff Engineer–level Leadership Craft story** and shows cross-team collaboration, modular architecture thinking, and de-risked rollout.
Here’s a **fully structured STAR answer** incorporating all the details you shared and some light polishing to make it interview-ready.

---

## **Leadership Craft Example – Walmart Labs Search Migration**

### **Situation & Problem**

At Walmart Labs, our **Role Management Portal** relied on database full-text search for user and role lookups.
This approach had several issues:

* **Slow performance:** P95 latency \~1.2s for common queries.
* **No typeahead support:** Admins had to type entire names/IDs.
* **High DB load:** Search queries impacted transactional performance.

We received **repeated feedback from portal users and product managers** that search was slowing down bulk operations, especially during peak onboarding periods.

---

### **Task**

I was responsible for **designing and implementing a new search system** that:

* Provided near real-time, typeahead search.
* Scaled independently from the transactional database.
* Could be rolled out safely without disrupting existing workflows.

---

### **Actions**

#### **1. Scoping & Estimation**

* Conducted **pain-point interviews** with product managers and real users to gather functional requirements (search by name, email, role, partial matches, ranking).
* Estimated the migration effort: schema design, data indexing, API changes, UAT testing, and rollout plan.
* Documented trade-offs between Solr vs Elasticsearch — recommended Elasticsearch for easier scaling, ecosystem maturity, and better typeahead support.

---

#### **2. Cross-Team Collaboration**

* Partnered with **Product** to prioritize search scenarios (user lookup first, followed by role search).
* Worked with **Portal Team** to agree on API contract changes and ensure backward compatibility.
* Coordinated with **DevOps** for provisioning ES clusters, monitoring, and alerting setup.

---

#### **3. Risk Identification & Mitigation**

| **Risk**                          | **Mitigation**                                                                         |
| --------------------------------- | -------------------------------------------------------------------------------------- |
| Incorrect results / missing users | Built backfill job in **Python** to index all historical data before go-live           |
| Rollout risk                      | Implemented **feature flags** — allowed quick rollback to DB search if issues detected |
| High ES cluster load              | Load-tested with 3× peak query volume before production                                |
| Data staleness                    | Set up incremental data sync pipeline (CDC events) to keep index fresh                 |

---

#### **4. Early Feedback & Incremental Delivery**

* Designed **modular search interface**, so the underlying provider (DB or ES) could be swapped just by changing config — enabling gradual rollout.
* Tested extensively in **UAT**, fixed edge-case bugs (special characters, partial matches, case sensitivity).
* Rolled out using feature flags:

  * 1% of users → 10% → 50% → 100%, while closely monitoring latency and accuracy metrics.
* Worked with QA and product teams to capture early feedback and refine search ranking before full deployment.

---

#### **5. Proving Success**

* **Metrics Tracked:**

  * P95 latency (target: <200ms)
  * Search result accuracy (compared DB vs ES responses)
  * ES cluster CPU/memory usage
* Shared weekly updates with stakeholders on rollout progress and impact.

---

### **Result**

* Reduced P95 search latency from **\~1.2 seconds to <200 ms**.
* Improved admin productivity (typeahead + partial matches cut search time by 40%).
* Removed significant query load from the transactional DB, improving overall system stability.
* Other teams adopted the **modular search interface** to integrate filtering APIs with Elasticsearch.
* Backfill pipeline became the standard pattern for future search indices in the org.

---

### **Reflection**

This project reinforced that **listening to end users first** and building **config-driven, pluggable architecture** allows not only safe rollout but also reusability across multiple teams. The feature flag + gradual rollout strategy significantly de-risked deployment and set a template for future infra upgrades.

---

### Why This Is Strong for a Staff-Level Interview

✅ **End-to-end ownership:** from requirements → design → infra setup → rollout → adoption
✅ **Cross-team collaboration:** product, portal team, DevOps, QA
✅ **Modular & scalable design:** pluggable provider interface, future-proofed
✅ **De-risked rollout:** feature flags, backfill jobs, staged ramp-up
✅ **Org-wide impact:** solution later reused by other APIs

********************************************************************************************************************************************************************************************************************************************************


2. Can you talk through a design decision that you made for this project, or other significant change you introduced to the tech stack/architecture? 
    * What tradeoffs did you make? What information did you use to assess this tradeoff? (Did you collect metrics, do a SPIKE, run experimentation, talk to the customers, etc)
    * Was there any conflict or a difference of opinion within the group?
    * How did you communicate the change? Did you have a process for feedback?
    * How did you plan for organic growth? What did you put in place to maintain the system health? What was your approach to maintaining high code quality?
    * How has this initiative changed the way you approach future projects? Would you do it again?

Perfect — this is a classic **Staff+ behavioral interview question** focusing on technical judgment, trade-offs, and long-term thinking.
Let’s use your **Walmart Labs search migration** as the base story (it’s rich with design decisions, trade-offs, conflicts, and future-proofing).

Here’s a polished **STAR answer** with extra depth, to sound like a Staff Engineer who balances technical excellence, collaboration, and business goals.

---

## **Behavioral Answer – Significant Design Decision (Walmart Labs Search Migration)**

### **Situation**

Our Role Management Portal relied on **database full-text search**, which caused high latency (\~1.2s P95), locked DB rows under load, and slowed down onboarding workflows.
We needed to improve latency, support typeahead, and reduce DB load — without breaking existing user workflows.

---

### **Design Decision & Trade-Offs**

**Decision:**
I proposed **introducing Elasticsearch** as a dedicated search backend, decoupling read traffic from the primary database.

**Trade-offs considered:**

| **Option**                           | **Pros**                                                               | **Cons**                                                      |
| ------------------------------------ | ---------------------------------------------------------------------- | ------------------------------------------------------------- |
| **Keep DB search, optimize queries** | No new infra; low migration cost                                       | Limited scalability, still adds DB load, no typeahead support |
| **Move to Solr**                     | Powerful search engine                                                 | Less cloud-native, harder to scale for our infra              |
| **Move to Elasticsearch (chosen)**   | Scalable, near real-time indexing, rich query DSL, typeahead supported | Requires new infra, data sync pipeline, learning curve        |

**How I assessed this trade-off:**

* Did a **spike** comparing Solr vs Elasticsearch — measured query latency, operational complexity, scaling cost.
* Collected **user pain points** (admin interviews + telemetry) to justify investment.
* Benchmarked both solutions on a sample dataset — Elasticsearch returned results in <200ms consistently.
* Proposed ES as future-proof for filtering & analytics use cases beyond just search.

---

### **Conflict & Resolution**

Some team members were concerned about:

* **Operational overhead:** Another service to maintain.
* **Data consistency:** ES index might lag behind DB.

I resolved this by:

* Prototyping a **change data capture pipeline** (using DB triggers + Kafka) to keep ES in sync in near real-time.
* Demonstrating a **blue/green deployment** plan with feature flags — so rollback was just a config change.
* Sharing metrics from the spike to show cost vs. latency benefits.
* Running a design review with portal team, DevOps, and product stakeholders to address their concerns.

---

### **Communication & Feedback Process**

* Documented the design in an **RFC**, including:

  * Architecture diagram (pluggable provider interface).
  * Rollout plan (1%, 10%, 50%, 100%).
  * Risk matrix and rollback strategy.
* Collected async comments on the RFC in Confluence.
* Ran **brown-bag session** to educate engineers on ES query DSL, index mapping, and monitoring.
* Incorporated feedback before building final implementation.

---

### **Planning for Organic Growth & System Health**

* Designed a **modular provider interface**: DBSearchProvider, ElasticsearchProvider — so future teams could add new providers without touching core code.
* Added **observability**: dashboards for query latency, index size, cluster health.
* Built **Python backfill script** to populate ES index in parallel before rollout.
* Wrote **automated tests** for ranking logic, null edge cases, and multi-field search.
* Implemented **alerting** for stale index detection and slow queries.

---

### **Result**

* Reduced search latency from \~1.2s → **<200ms P95**.
* Improved admin productivity significantly (typeahead cut search time by 40%).
* Removed major load from DB, reducing lock contention and improving overall portal stability.
* Other teams adopted the pluggable search provider design for their filtering APIs.

---

### **Reflection**

This initiative changed my approach to system design:

* **Customer-first design:** I now always start by talking to users and measuring pain (telemetry + interviews).
* **Risk-managed rollout:** Feature flags + gradual ramp-up + rollback hooks are now default in my playbook.
* **Reusable architecture:** I think about how to design components so they can be reused by other teams.
* **Metrics-driven decisions:** Benchmarks and A/B tests are part of every significant tech stack change.

Would I do it again? **Yes, absolutely.** But next time I would also add synthetic load replay in staging earlier to catch edge cases before UAT.

---

### Why This Is a Strong Staff+ Answer

✅ **Shows technical judgment:** compared alternatives, collected data, ran spike
✅ **Shows leadership:** design reviews, cross-team alignment, mentoring
✅ **Shows risk-awareness:** feature flags, rollback strategy, CDC pipeline
✅ **Shows future-proofing:** modular design, dashboards, test coverage
✅ **Shows growth:** explicitly shares what you learned and how it shaped your future approach

************************************************************************************************************************************************************************************************************************************************
3. How did you share knowledge with other members of the project or support their career growth?
    * How did you approach delegating work?
    * How did you provide constructive feedback to other engineers?
    * What did you do to make sure your communication was effective for your teammates?

Absolutely — here’s a **crisp one-pager leadership cheat sheet** you can use as a quick reference before your Staff+ interviews.
It highlights **mentorship, delegation, feedback, and communication style** — tailored from your Swiggy and Walmart projects.

---

# **Leadership Cheat Sheet – Manish Kumar (Staff Engineer)**

### **1️⃣ Knowledge Sharing & Mentorship**

* **Architecture Walkthroughs:** Led design reviews & brown-bag sessions to explain service boundaries, API contracts, feature flags, and rollout plans.
* **Documentation:** Authored onboarding guides, API specs, and monitoring playbooks — became reference material for new hires and other teams.
* **Pairing & Upskilling:** Paired with junior engineers initially, then let them drive features independently (progressive autonomy).

---

### **2️⃣ Delegation Style**

* **Break Down Work:** Split projects into independent deliverables (infra, APIs, tests, backfill jobs).
* **Match to Skills & Goals:**

  * Junior engineers → lower-risk tasks (dashboards, configs) to build confidence.
  * Senior engineers → critical paths (schema design, dual-write logic).
* **Progressive Ownership:** Start with guidance, transition to full ownership + reviews.

---

### **3️⃣ Constructive Feedback**

* **PR Reviews as Teaching Tool:** Explain *why* behind design patterns (retries, circuit breakers, idempotency).
* **Balanced Approach:** Highlight good work + give clear, actionable feedback (“wrap DB errors in a standard response to improve observability”).
* **1:1 Mentorship:** Discuss growth areas privately and celebrate wins publicly.

---

### **4️⃣ Communication Practices**

* **Transparent Updates:** Daily stand-up summaries, shared Slack channels for visibility.
* **Visual Thinking:** Use sequence diagrams and rollout flow charts to align engineers, PMs, and SREs.
* **Async First:** Document decisions in Confluence/RFCs for long-term reference.
* **Open Door:** Encourage questions during reviews to surface hidden assumptions early.

---

### **5️⃣ System Health & Long-Term Thinking**

* Build **feature flags**, dashboards, and alerting from day one.
* Plan for **incremental rollout** (1% → 10% → 50% → 100%) and rollback path.
* Focus on **reusable architecture & documentation** to help other teams replicate success.

---

### **Leadership Mindset**

> **“Scale impact by enabling others.”**
> My role is to design systems that last, de-risk large changes, and grow engineers around me so the team can move faster independently.


****************************************************************************************************************************************************************************************
4. Tell me about a time where you identified a need and made a cultural or behavioural change within your organisation?
    * How did you identify the need for this change?
    * What role did you play in implementing the change?
    * Did your team require support navigating this change? If so, how did you manage this?
    * What did you learn?
